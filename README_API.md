# ğŸ“¡ Binance Spark Analytics - Mode API

## ğŸ¯ Analyse Crypto en Temps RÃ©el

**SystÃ¨me Spark qui analyse 413 cryptomonnaies USDT avec 100+ alertes critiques automatiques**

### ğŸ—ï¸ Architecture
```
API Binance â†’ Spark Analytics â†’ MongoDB â†’ Dashboard
   3279 cryptos    413 USDT pairs    Collections    Visualisation
   temps rÃ©el      analysÃ©es         structurÃ©es    interactive
```

## ğŸš€ Utilisation - 3 Ã‰tapes Simples

### **Ã‰tape 1 : DÃ©marrer l'Infrastructure**
```bash
# Lancer Hadoop + MongoDB
docker-compose -f compose.yml up -d

# VÃ©rifier que tout fonctionne
docker ps
```

### **Ã‰tape 2 : Lancer l'Analyse Spark**
```bash
# Analyse complÃ¨te des cryptos
python binance_spark_analytics.py

# RÃ©sultats attendus :
# 413 USDT pairs analyzed
# 100+ critical alerts detected  
# $20+ billion volume processed
# Results saved to MongoDB
```

### **Ã‰tape 3 : Voir le Dashboard**
```bash
# Dashboard interactif
streamlit run visualization/dashboard/dashboard.py

# Ouvrir : http://localhost:8501
```

## ğŸ“Š RÃ©sultats Typiques

```
 ANALYSIS RESULTS:
Total symbols analyzed: 413
Critical alerts: 101
Average price change: 1.84%
Total volume: $20,651,192,158

 ANALYSIS COMPLETED SUCCESSFULLY!
```

## ğŸ” Ce que Spark Analyse

### **DonnÃ©es TraitÃ©es**
- **3279 cryptos** rÃ©cupÃ©rÃ©es depuis l'API Binance
- **413 paires USDT** filtrÃ©es et nettoyÃ©es
- **$20+ milliards** de volume analysÃ©

### **Indicateurs CalculÃ©s**
- **VolatilitÃ©** : Ã‰cart prix haut/bas
- **Momentum** : Variation 24h
- **Score de risque** : Composite volatilitÃ© + spread
- **Signaux trading** : BUY/SELL/HOLD
- **Niveaux d'anomalie** : NORMAL â†’ CRITICAL

### **Alertes GÃ©nÃ©rÃ©es**
- ğŸš¨ **Anomalies critiques** : Mouvements suspects
- ğŸ“ˆ **Signaux d'achat** : OpportunitÃ©s dÃ©tectÃ©es
- ğŸ“‰ **Signaux de vente** : Risques identifiÃ©s
- âš ï¸ **Recommandations** : BUY/SELL/AVOID/MONITOR

## ğŸ“Š Dashboard Interactif

### **FonctionnalitÃ©s**
- **Vue d'ensemble** : MÃ©triques temps rÃ©el
- **Analyse des risques** : Distribution par catÃ©gories
- **Signaux de trading** : RÃ©partition BUY/SELL/HOLD
- **Anomalies dÃ©tectÃ©es** : Alertes critiques
- **Top gainers/losers** : Performance du marchÃ©

### **AccÃ¨s**
- **URL** : http://localhost:8501
- **Actualisation** : Bouton "Actualiser les DonnÃ©es"
- **Filtres** : Par risque, signal, anomalie

## ğŸ› ï¸ Configuration

### **Ports UtilisÃ©s**
- `27018` : MongoDB
- `8501` : Dashboard Streamlit
- `9870` : Hadoop NameNode (interface web)

### **Base de DonnÃ©es MongoDB**
- **Nom** : `binance_analytics`
- **Collections** :
  - `processed_data` : DonnÃ©es analysÃ©es
  - `critical_alerts` : Alertes critiques
  - `market_metrics` : Statistiques globales

## ğŸ¯ Avantages du Mode API

âœ… **RapiditÃ©** : Analyse en 30 secondes  
âœ… **Temps rÃ©el** : DonnÃ©es les plus rÃ©centes  
âœ… **SimplicitÃ©** : Une seule commande  
âœ… **FiabilitÃ©** : Pas de dÃ©pendance HDFS  
âœ… **Performance** : Traitement direct  

## ğŸ”§ DÃ©pannage

### **ProblÃ¨me : "Connection refused MongoDB"**
```bash
# RedÃ©marrer MongoDB
docker restart db
```

### **ProblÃ¨me : "API timeout"**
```bash
# Relancer l'analyse
python binance_spark_analytics.py
```

### **ProblÃ¨me : "Dashboard ne s'ouvre pas"**
```bash
# VÃ©rifier le port
netstat -an | grep 8501

# Relancer Streamlit
streamlit run visualization/dashboard/dashboard.py --port 8502
```

## ğŸ‰ RÃ©sumÃ©

**Mode API = Solution parfaite pour :**
- Analyses rapides et temps rÃ©el
- Tests et dÃ©veloppement  
- DÃ©monstrations
- Production lÃ©gÃ¨re

**Une seule commande pour analyser 413 cryptos avec Spark !** ğŸš€
- **Actualisation** : Bouton "Actualiser les DonnÃ©es"
- **Filtres** : Par risque, signal, anomalie

## ğŸ”§ Configuration

### **Ports UtilisÃ©s**
- `27018` : MongoDB (externe)
- `8501` : Dashboard Streamlit
- `9870` : Hadoop NameNode
- `9000` : Hadoop HDFS

### **Base de DonnÃ©es**
- **MongoDB** : `binance_analytics`
- **Connexion** : `mongodb://localhost:27018/`

## ğŸ¯ Utilisation AvancÃ©e

### **Pipeline ETL Complet (avec Spark)**
```bash
# Utiliser le pipeline ETL avec HDFS
python processing/pipelines/pipeline_etl.py --hdfs-path /binance/raw/data.json
```

### **Jobs Spark Individuels**
```bash
# Test du nettoyage
python processing/spark_jobs/cleaning.py

# Test des indicateurs
python processing/spark_jobs/indicators.py

# Test des anomalies
python processing/spark_jobs/anomalies.py
```

## ğŸ“Š Exemples de RÃ©sultats

### **Analyse Typique**
```
ğŸ“Š ANALYSIS RESULTS:
Total symbols analyzed: 413
Critical alerts: 99
Average price change: 1.07%
Total volume: $21,315,935,388

Risk distribution:
LOW: 262, MEDIUM: 140, HIGH: 8, VERY_HIGH: 3

Signal distribution:
HOLD: 361, BUY: 40, SELL: 11, STRONG_BUY: 1

Top gainers: BARDUSDT (+162%), WUSDT (+21%)
Top losers: PROMUSDT (-8%), PUMPUSDT (-7%)
```

### **Alertes Critiques**
```
ğŸš¨ CRITICAL ALERTS:
BTCUSDT: AVOID - CRITICAL anomaly
ETHUSDT: AVOID - CRITICAL anomaly
BARDUSDT: CAUTION - HIGH anomaly (+162%)
```

## ğŸ› ï¸ Maintenance

### **RedÃ©marrer l'infrastructure**
```bash
docker-compose -f compose.yml down
docker-compose -f compose.yml up -d
```

### **Nettoyer les donnÃ©es**
```bash
# Se connecter Ã  MongoDB et vider les collections
docker exec -it db mongo binance_analytics --eval "db.processed_data.deleteMany({})"
```

### **Logs et Debug**
```bash
# Voir les logs des containers
docker logs namenode
docker logs db

# Logs Python avec niveau DEBUG
export PYTHONPATH=.
python -c "import logging; logging.basicConfig(level=logging.DEBUG)"
```

## ğŸ‰ RÃ©sumÃ©

Ce systÃ¨me vous permet d'analyser **automatiquement** les marchÃ©s crypto avec :
- âœ… **DÃ©tection d'anomalies** en temps rÃ©el
- âœ… **Signaux de trading** automatiques
- âœ… **Dashboard interactif** pour visualisation
- âœ… **Architecture Spark** scalable
- âœ… **Alertes critiques** pour Ã©viter les piÃ¨ges

**Votre mission Spark est complÃ¨te et opÃ©rationnelle !** ğŸš€
