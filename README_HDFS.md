# ğŸ—„ï¸ Binance Spark Analytics - Mode HDFS

## ğŸ¯ Guide Complet pour RÃ©cupÃ©rer les DonnÃ©es depuis HDFS

Ce guide vous explique comment utiliser **HDFS (Hadoop Distributed File System)** pour stocker et rÃ©cupÃ©rer les donnÃ©es Binance avec Spark.

## ğŸ—ï¸ Architecture HDFS

```
API Binance â†’ Stockage HDFS â†’ Spark Analytics â†’ MongoDB â†’ Dashboard
     â†“              â†“              â†“             â†“         â†“
  DonnÃ©es JSON   Fichiers dans   Traitement    RÃ©sultats  Visualisation
   temps rÃ©el     /binance/raw/   distribuÃ©    structurÃ©s interactive
```

## ğŸ“ Structure HDFS

```
HDFS:/
â””â”€â”€ binance/
    â””â”€â”€ raw/
        â”œâ”€â”€ binance_20250119_120000.json
        â”œâ”€â”€ binance_20250119_130000.json
        â””â”€â”€ binance_20250119_140000.json
```

## ğŸš€ Ã‰tapes pour Utiliser HDFS

### **1. ğŸ“¦ Stocker les DonnÃ©es dans HDFS**

#### **MÃ©thode A : Script Automatique**
```bash
# CrÃ©er et exÃ©cuter le gestionnaire HDFS
python hdfs_manager.py store
```

#### **MÃ©thode B : Commandes Manuelles**
```bash
# 1. CrÃ©er le rÃ©pertoire HDFS
docker exec namenode hdfs dfs -mkdir -p /binance/raw

# 2. RÃ©cupÃ©rer les donnÃ©es et les copier
# (Voir section "Commandes HDFS DÃ©taillÃ©es" ci-dessous)
```

### **2. ğŸ” VÃ©rifier les Fichiers HDFS**

```bash
# Lister les fichiers
docker exec namenode hdfs dfs -ls /binance/raw/

# Voir le contenu d'un fichier (premiÃ¨res lignes)
docker exec namenode hdfs dfs -cat /binance/raw/binance_*.json | head -20

# Informations sur un fichier
docker exec namenode hdfs dfs -stat /binance/raw/binance_*.json
```

### **3. ğŸ”¥ Analyser avec Spark depuis HDFS**

```bash
# MÃ©thode recommandÃ©e : Workflow complet HDFS â†’ Spark
python hdfs_to_spark.py

# Ou Ã©tapes manuelles :
# 1. Stocker dans HDFS
python hdfs_direct.py

# 2. RÃ©cupÃ©rer et analyser
python hdfs_to_spark.py
```

## ğŸ› ï¸ Commandes HDFS DÃ©taillÃ©es

### **Stockage des DonnÃ©es**

```bash
# 1. RÃ©cupÃ©rer les donnÃ©es Binance (script Python)
python -c "
import requests, json, datetime
data = requests.get('https://api.binance.com/api/v3/ticker/24hr').json()
timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
with open(f'binance_{timestamp}.json', 'w') as f:
    json.dump(data, f, indent=2)
print(f'Data saved: binance_{timestamp}.json')
"

# 2. Copier vers le container namenode
docker cp binance_*.json namenode:/tmp/

# 3. DÃ©placer vers HDFS
docker exec namenode hdfs dfs -put /tmp/binance_*.json /binance/raw/

# 4. Nettoyer le fichier temporaire
docker exec namenode rm /tmp/binance_*.json
rm binance_*.json
```

### **Gestion des Fichiers HDFS**

```bash
# Lister tous les fichiers
docker exec namenode hdfs dfs -ls -R /binance/

# Voir la taille des fichiers
docker exec namenode hdfs dfs -du -h /binance/raw/

# Supprimer un fichier
docker exec namenode hdfs dfs -rm /binance/raw/binance_old.json

# Supprimer tous les fichiers anciens
docker exec namenode hdfs dfs -rm /binance/raw/binance_2025011*.json

# Copier un fichier depuis HDFS vers local
docker exec namenode hdfs dfs -get /binance/raw/binance_latest.json /tmp/
docker cp namenode:/tmp/binance_latest.json ./
```

### **Surveillance HDFS**

```bash
# Statut du systÃ¨me HDFS
docker exec namenode hdfs dfsadmin -report

# VÃ©rifier l'intÃ©gritÃ© des fichiers
docker exec namenode hdfs fsck /binance/raw/

# Espace disponible
docker exec namenode hdfs dfs -df -h /
```

## ğŸ”§ Script Gestionnaire HDFS

### **Utilisation du Gestionnaire HDFS**

```bash
# Stocker de nouvelles donnÃ©es dans HDFS
python hdfs_manager.py store

# Lister les fichiers HDFS
python hdfs_manager.py list

# Lire un fichier spÃ©cifique
python hdfs_manager.py read binance_20250119_120000.json
```

## ğŸ”¥ Workflow Complet HDFS â†’ Spark

### **1. PrÃ©paration**
```bash
# DÃ©marrer l'infrastructure
docker-compose -f compose.yml up -d

# VÃ©rifier que HDFS est actif
docker exec namenode hdfs dfsadmin -safemode get
```

### **2. Stockage des DonnÃ©es**
```bash
# MÃ©thode automatique
python hdfs_manager.py store

# Ou mÃ©thode manuelle
python -c "
import requests, json, datetime
data = requests.get('https://api.binance.com/api/v3/ticker/24hr').json()
timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
with open(f'binance_{timestamp}.json', 'w') as f:
    json.dump(data, f, indent=2)
print(f'binance_{timestamp}.json')
"

# Copier vers HDFS
docker cp binance_*.json namenode:/tmp/
docker exec namenode hdfs dfs -mkdir -p /binance/raw
docker exec namenode hdfs dfs -put /tmp/binance_*.json /binance/raw/
```

### **3. Analyse Spark depuis HDFS**
```bash
# Lancer l'analyse Spark
python binance_spark_analytics.py --hdfs

# RÃ©sultats attendus :
# âœ… 413 USDT pairs analyzed
# âœ… 99 critical alerts detected
# âœ… $20+ billion volume processed
# âœ… Results saved to MongoDB
```

### **4. Visualisation**
```bash
# Dashboard interactif
streamlit run visualization/dashboard/dashboard.py
```

## ğŸš¨ RÃ©solution de ProblÃ¨mes HDFS

### **ProblÃ¨me : "No FileSystem for scheme 'C'"**
```bash
# Solution : Utiliser les commandes Docker exec
docker exec namenode hdfs dfs -ls /
```

### **ProblÃ¨me : "Connection refused namenode:9000"**
```bash
# VÃ©rifier que namenode est actif
docker ps | grep namenode

# RedÃ©marrer si nÃ©cessaire
docker restart namenode
```

### **ProblÃ¨me : "Permission denied"**
```bash
# DÃ©sactiver les permissions HDFS (dÃ©jÃ  fait dans hadoop.env)
docker exec namenode hdfs dfs -chmod -R 777 /binance/
```

### **ProblÃ¨me : "Safe mode is ON"**
```bash
# DÃ©sactiver le mode safe
docker exec namenode hdfs dfsadmin -safemode leave
```

## ğŸ“Š Comparaison des Modes

| Mode | Source | Avantages | InconvÃ©nients |
|------|--------|-----------|---------------|
| **API** | Direct Binance | âš¡ Rapide, temps rÃ©el | ğŸ”„ Pas de persistance |
| **JSON File** | Fichier local | ğŸ’¾ Persistant, testable | ğŸ“ Stockage limitÃ© |
| **HDFS** | Hadoop distribuÃ© | ğŸ—ï¸ Scalable, distribuÃ© | ğŸ”§ Configuration complexe |

## ğŸ¯ Recommandations

### **Pour le DÃ©veloppement :**
```bash
# Mode JSON File (simulation HDFS)
python binance_spark_analytics.py --json-file temp_data/binance_*.json
```

### **Pour la Production :**
```bash
# Mode HDFS (architecture complÃ¨te)
python hdfs_manager.py store
python binance_spark_analytics.py --hdfs
```

### **Pour les Tests Rapides :**
```bash
# Mode API (temps rÃ©el)
python binance_spark_analytics.py
```

## ğŸ‰ RÃ©sumÃ©

Vous avez maintenant **3 mÃ©thodes** pour alimenter votre pipeline Spark :

1. **ğŸ“¡ API Mode** : DonnÃ©es temps rÃ©el directes
2. **ğŸ“ JSON File Mode** : Simulation HDFS avec fichiers locaux  
3. **ğŸ—„ï¸ HDFS Mode** : Architecture Big Data complÃ¨te

**Toutes les mÃ©thodes produisent les mÃªmes rÃ©sultats d'analyse Spark : 413 cryptos, 99 alertes critiques, $20+ milliards de volume !** ğŸš€
